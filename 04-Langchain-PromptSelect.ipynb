{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e89d5cd",
   "metadata": {},
   "source": [
    "# Prompt Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3daa962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "# 創建反義詞任務\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"hot\", \"output\": \"cold\"},\n",
    "    {\"input\": \"up\", \"output\": \"down\"},\n",
    "    {\"input\": \"big\", \"output\": \"small\"},\n",
    "    {\"input\": \"fast\", \"output\": \"slow\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\"\n",
    ")\n",
    "\n",
    "examples_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=50,  # 設定最大長度\n",
    ")\n",
    "\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=examples_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Please provide the antonym for the following word:\\n\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efea179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the antonym for the following word:\n",
      "\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: hot\n",
      "Output: cold\n",
      "\n",
      "Input: up\n",
      "Output: down\n",
      "\n",
      "Input: big\n",
      "Output: small\n",
      "\n",
      "Input: fast\n",
      "Output: slow\n",
      "\n",
      "Input: happy\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt.format(adjective=\"happy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793954f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the antonym for the following word:\n",
      "\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: hot\n",
      "Output: cold\n",
      "\n",
      "Input: up\n",
      "Output: down\n",
      "\n",
      "Input: big\n",
      "Output: small\n",
      "\n",
      "Input: fast\n",
      "Output: slow\n",
      "\n",
      "Input: This is a long and short, big but small article.\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "long_text = \"This is a long and short, big but small article.\"\n",
    "print(dynamic_prompt.format(adjective=long_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "509898c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the antonym for the following word:\n",
      "\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: hot\n",
      "Output: cold\n",
      "\n",
      "Input: up\n",
      "Output: down\n",
      "\n",
      "Input: big\n",
      "Output: small\n",
      "\n",
      "Input: fast\n",
      "Output: slow\n",
      "\n",
      "Input: light\n",
      "Output: heavy\n",
      "\n",
      "Input: light\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "new_examples = {\"input\": \"light\", \"output\": \"heavy\"}\n",
    "dynamic_prompt.example_selector.add_example(new_examples)\n",
    "print(dynamic_prompt.format(adjective=\"light\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e0eada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9081781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The antonym for the word \"light\" is actually \"dark\", not \"heavy\". Heavy and dark are related but distinct concepts.\\n\\nSo, the corrected output would be:\\n\\n Input: light\\nOutput: dark'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "openai_api_key = os.getenv(\"EMPTY\")\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.7,\n",
    "    openai_api_base=\"http://127.0.0.1:11434/v1\",\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = dynamic_prompt | chat | output_parser\n",
    "\n",
    "chain.invoke({\"adjective\": \"light\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de01445b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Path D:\\ai\\downloads\\bge-large-zh-v1.5 not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhuggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[32m      6\u001b[39m embeddings_path = \u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mai\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mdownloads\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mbge-large-zh-v1.5\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m embeddings = \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m example_prompt = PromptTemplate(\n\u001b[32m     10\u001b[39m     input_variables=[\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     11\u001b[39m     template=\u001b[33m\"\u001b[39m\u001b[33mInput: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOutput: \u001b[39m\u001b[38;5;132;01m{output}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m examples = [\n\u001b[32m     15\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mhappy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msad\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     16\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mhot\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcold\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfast\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mslow\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     20\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gostj\\Desktop\\AI_nextgen\\Langchain_2025_twh\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:222\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    220\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    221\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gostj\\Desktop\\AI_nextgen\\Langchain_2025_twh\\.venv\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:92\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import sentence_transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43msentence_transformers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gostj\\Desktop\\AI_nextgen\\Langchain_2025_twh\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:304\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(model_name_or_path):\n\u001b[32m    302\u001b[39m     \u001b[38;5;66;03m# Not a path, load from hub\u001b[39;00m\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mor\u001b[39;00m model_name_or_path.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPath \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mand\u001b[39;00m model_name_or_path.lower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m basic_transformer_models:\n\u001b[32m    307\u001b[39m         \u001b[38;5;66;03m# A model from sentence-transformers\u001b[39;00m\n\u001b[32m    308\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Path D:\\ai\\downloads\\bge-large-zh-v1.5 not found"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector import (MaxMarginalRelevanceExampleSelector, SemanticSimilarityExampleSelector) \n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_path = \"D:\\\\ai\\\\downloads\\\\bge-large-zh-v1.5\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_path)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\"\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"hot\", \"output\": \"cold\"},\n",
    "    {\"input\": \"up\", \"output\": \"down\"},\n",
    "    {\"input\": \"big\", \"output\": \"small\"},\n",
    "    {\"input\": \"fast\", \"output\": \"slow\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d52e4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain_2025_twh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
